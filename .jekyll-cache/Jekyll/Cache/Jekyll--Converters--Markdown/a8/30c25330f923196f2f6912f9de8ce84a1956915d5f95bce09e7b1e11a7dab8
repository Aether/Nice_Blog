I"<p>构建决策树之通常包括三个步骤：</p>

<ol>
  <li>特征选择</li>
  <li>决策树生成</li>
  <li>决策树剪枝</li>
</ol>

<h2 id="决策树生成">决策树生成</h2>

<h3 id="信息增益">信息增益</h3>

<p>通常使用<strong>信息熵</strong>Information Entropy度量样本集合纯度。样本集合为$D$，第$k$类样本所占比例为$p_k(k=1,2,…,N)$，则样本$D$的信息熵为</p>

<script type="math/tex; mode=display">H(D) =-\sum\limits_{k=1}^{N}p_k{\log}_2p_k</script>

<p>$Ent(D)$越小，$D$的纯度越高。</p>

<p>设离散属性$a$有$V$个可能的取值，当用属性$a$对样本集合$D$进行划分所获的的<strong>信息增益</strong>Information Gain为目标类变量与属性<script type="math/tex">a</script>变量在<script type="math/tex">D</script>(样本集)上的<strong>互信息</strong>。</p>

<script type="math/tex; mode=display">Gain(D,a)=H(D)- \sum \limits_{v=1}^V\frac{|D^v|}{|D|}H(D^v)</script>

<p>一般而言，信息增益越大，意味着使用属性$a$进行划分获得的纯度提升越大。</p>

<p>ID3选择属性使用信息增益</p>

<script type="math/tex; mode=display">a_* = \mathop{ \arg \max}\limits_{a \in A} Gain(D,a)</script>

<h3 id="增益率">增益率</h3>

<p>信息增益更偏好于<strong>可取更多的值属性</strong>，为减小这种偏好的影响，在C4.5选择属性使用<strong>增益率</strong></p>

<script type="math/tex; mode=display">Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}</script>

<p>$IV(a)$为属性$a$的<strong>固有值</strong>Intrinsic Value。属性$a$可取值越多，$IV(a)$的值通常会越大。</p>

<script type="math/tex; mode=display">IV(a)=-\sum \limits_{v=1}^V\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}</script>

<p>个人理解$IV(a)$即计算按属性$a$分类时的信息熵，属性$a$多时混乱程度通常更大，导致信息增益更大。</p>

<p>C4.5先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益率最高的。</p>

<h3 id="基尼指数">基尼指数</h3>

<p>CART决策树使用<strong>基尼指数</strong>选择划分属性，数据集$D$的纯度可用<strong>基尼值</strong>度量</p>

<script type="math/tex; mode=display">Gini(D)=\sum_{k=1}^N\sum_{k'\neq k}p_kp_{k'} = 1-\sum_{k=1}^N p_k^2</script>

<p>直观来说，$Gini(D)$反映了从数据集$D$中随机抽取两个样本，其类别标记不一致的概率，因此$Gini(D)$越小，则数据集$D$纯度越高。</p>

<p>$Gini$在节点取值分布均匀时取最大值$1-1/V$，在只包含一个类别时取最小值$0$。
属性$a$的基尼指数定义为</p>

<script type="math/tex; mode=display">𝐺𝑖𝑛𝑖\_𝑖𝑛𝑑𝑒𝑥(𝐷,𝑎)=\sum \limits_{𝑣=1}^𝑉\frac{|D^v|}{|D|}𝐺𝑖𝑛𝑖(𝐷^𝑣)</script>

<script type="math/tex; mode=display">a_* = \mathop{ \arg \min}\limits_{a \in A} Gini\_index(D,a)</script>

<hr />

<h2 id="决策树剪枝">决策树剪枝</h2>

<p><strong>剪枝</strong>pruning是决策树学习算法中避免过拟合的主要手段。</p>

<h3 id="预剪枝">预剪枝</h3>

<p>​		在决策树生成过程中，对每个结点划分前进行估计，若对此结点划分不能带来决策树泛化性能的提升，则停止划分并标记为叶子结点。</p>

<p>​		预剪枝使得决策树的很多分支都没有展开，这不仅降低了过拟合的风险，还显著减少了决策树训练和预测的时间开销。</p>

<p>​		但是另一方面，有些分支的当前划分虽然不能带来泛化性能的提升，甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致泛化性能显著上升。所以基于贪心策略的预剪枝可能带来欠拟合的风险。</p>

<h3 id="后剪枝">后剪枝</h3>

<p>首先生成完整的决策树，然后自下而上地逐层剪枝，如果一个节点的子节点被删除后，决策树的准确度没有降低，那么就将该节点设置为叶节点。训练时间开销大。</p>

<hr />

<h2 id="连续和缺失值">连续和缺失值</h2>

<h3 id="属性值连续">属性值连续</h3>

<p>对于连续的属性而言，属性值可取分界点后构成二分法的分类。分界点的取值同样是两点的中点值，对中点值进行分类增益计算后再进行比较。连续值在子节点上依然能够再次进行分类。</p>

<h3 id="样本值缺失">样本值缺失</h3>

<p>对于训练决策树时训练集存在某些特征的属性值缺失的情况，会遇到如下两个问题：</p>

<ol>
  <li>如何选择最优属性进行划分</li>
  <li>给定划分属性，若在该属性上值缺失，如何确定样本的归属</li>
</ol>

<p>对于问题1而言：只要将所有的计算换成无缺样本的数据即可。最终增益乘以对应特征所含有值的概率值，</p>

<p>对于问题2而言：将缺失值的样本按照一定的比例以不同的概率划分到不同的子节点中去。若取值已知，则划分到对应的子节点中去，样本权值在子节点中保持为wx，若取值未知，则将x同时划入所有的子节点中区，并将样本去找你之在对应中调整为对应的比例乘以wx，权重在之后则以概率的形式体现到计算中去。</p>

<hr />

<h2 id="多变量决策树">多变量决策树</h2>

<p>多变量决策树的学习过程中，不是为每个非叶节点寻找一个最优划分属性，而是试图建立一个合适的线性分类器于一个节点中，减缓决策树的复杂度。</p>
:ET